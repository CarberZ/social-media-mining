1. find 2 largest group of dog and cat on douban
	cat:	337238
	dog:	156360
2. get the user id of each member and corresponding location in China, use d3.js to show the result
	cat: 10k
	dog: 10k
3. for each member, find the top 180 books and movies he/shi likes the most corresponding to the rating, and the number of users he/she follows, his/her followers and the group he/she likes to go.
4. for each book, find the most popular tag people use
5. for each movie, find at most 3 types that movie belongs to, 37 types, the speed is limited to 40 times/minute
6. build the feature vector for each book and movie with dataframe of pandas
7. add corresponding feature up for each user and do normalization
	randomly choose 2000 users from each group
8. use multiple machine learning techniques to do data mining, try to find a pattern to classify different groups
9. use SVD to do dimension reduction to avoid the curse of dimensionality, for movies, the dimension is reduced to 23 dimensions
9. after using logistic regression, decision tree, SVM, naive bayes, k nearest neighbour, boosting algorithm for 10 times with the split of 20% of the dataset to be the test data, 80% to be the training data, the average result is:


logistic regression score:  0.5050761421319797
ldecision tree score:  0.5406091370558376
SVM score:  0.5329949238578681
Naive bayes score:  0.5304568527918782
k nearest neighbour score:  0.5228426395939086
boosting score:  0.5126903553299492

17.45% information kept


so the assumption that cat people and dog people loves different kinds of movies are not true

the result for following and followers:

logistic regression score:  0.5488013698630136
ldecision tree score:  0.502568493150685
SVM score:  0.5282534246575342
Naive bayes score:  0.4965753424657534
k nearest neighbour score:  0.5368150684931506
boosting score:  0.5547945205479452

if I put them together:
2D mapping-96.5% information keep


logistic regression score:  0.5203045685279187
ldecision tree score:  0.4743147208121827
SVM score:  0.5076142131979695
Naive bayes score:  0.5177664974619289
k nearest neighbour score:  0.4896954314720812
boosting score:  0.5279187817258884


4000 sample in twitter:

logistic regression score:  0.5393419170243204
decision tree score:  0.5321888412017167
SVM score:  0.49642346208869814
Naive bayes score:  0.49356223175965663
k nearest neighbour score:  0.5035765379113019
boosting score:  0.5221745350500715






10. the sentiment analysis:
	cat people are more likely to be negative than dog people





